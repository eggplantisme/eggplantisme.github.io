---
layout: post
title: 图卷积网络-笔记
tags: [Algorithm]
date: 2021-11-21 18:46
toc: true
---

## 图卷积网络

图神经网络一开始是按照RNN的方式进行研究，用RNN的方式处理网络上的数据。在CNN流行之后，思考如何在图上进行卷积。即图卷积网络(GCN)。关于GCN有两个方向，一个是基于谱的方法，一个基于图的方法，类似于图像处理中的频域和空间域。本文主要讲频域上的处理，基于论文2016 - Semi-Supervised Classification with Graph Convolutional Networks - Kipf, Welling，以及一个知乎上的讲解：[如何理解 Graph Convolutional Network（GCN）？](https://www.zhihu.com/question/54504471/answer/332657604)

卷积操作在图像上很有用，可以提取图像的特征，但是在图上就有很多限制，原因在于图和图像的数据结构不一样，图像是一个Euclidean Structure，图像的数据是排列整齐的矩阵，但是图上的数据是Non Euclidean Structure，每一个节点的邻居的数量不是固定的，所以很难直接像图像卷积一样在图上做卷积。但是可以类似图像上的傅里叶变换以及卷积定理，将空间域的卷积转换为频域(图上叫做谱)的乘积。但是面临的的问题就是，图上的傅里叶变换是什么，从而需要了解图信号处理(graph signal processing)以及谱图理论(spectral graph theory)，这里需要一个核心的概念就是拉普拉斯矩阵L。

#### 拉普拉斯矩阵

拉普拉斯矩阵有三种形式:

$$
\begin{aligned}
L &= D - A\\
L &= D^{-1/2}(D-A)D^{-1/2}\\
L &= D^{-1}(D-A)
\end{aligned}
$$

其有很好的性质：

- L是实对称矩阵，意味着L有n个线性无关的特征向量，可以进行特征分解，这些特征向量可以化成两两正交的特征向量，形成正交矩阵。
- L是半正定矩阵，所以L的特征值一定非负。

对L进行特征分解得到:

$$
\begin{aligned}
L &= U\Lambda U^{-1} = U\Lambda U^{T}\\
其中, U &= (u_1, u_2, ..., u_n), \Lambda = diag([\lambda_1, \lambda_2, ..., \lambda_n])
\end{aligned}
$$

设L的多个特征值

$$
0 = \lambda_1 < \lambda_2 \leq ... \leq \lambda_n = \lambda_{max}
$$

L的特征值可以看作频率，其对应的特征向量可以看作该频率下网络上的图信号graph signal。网络的图信号意思是网络每一个节点上对应一个值，这些值合起来表示的一个向量就是graph signal。特征值越大（即频率越大）的特征值对应的特征向量（一个图信号）看起来就会更震荡，反之这个信号就会更平稳，如图（来自2013 - The emerging field of signal processing on graphs Extending high-dimensional data analysis to networks and other irregular domain）：

![graph_signal](../../../../assets/img/algorithm/graph_signal.PNG)

换一个方法，我们需要知道特征方程

$$
Lu=\lambda u
$$

特征值为0时，根据L的定义可以得出此时特征向量u为全1的向量，这样的向量自然如上图第一个图一样，非常平滑，变化的频率为0也理所应当。

由于U构成了n维空间的一个正交基，那么这个图上的任意一个信号f都可以根据U进行分解：

$$
\begin{aligned}
f=\hat{f}(\lambda_1)u_1 + \hat{f}(\lambda_2)u_2 + ... + \hat{f}(\lambda_n)u_n\\
\hat{f} 为f的傅里叶变换。
\end{aligned}
$$

这样就形成了谱和空间域的对应，图信号的傅里叶变换为拉普拉斯矩阵特征值表示的频率上的向量

$$
f\xrightarrow[]{F}\hat{f}
$$

那么图上的傅里叶变换应该怎么做。

#### 图上的傅里叶变换

普通的傅里叶变换如下：

$$
\begin{aligned}
F(w)&=\int f(t)e^{-iwt}dt\\
e^{-iwt}&是拉普拉斯算子的特征函数，满足特征方程。
\end{aligned}
$$

广义的特征方程为：

$$
AV = \lambda V
$$

$$
\begin{aligned}
&A 表示一种变换，包括算子，矩阵。\\

&V 表示特征向量，函数可以看作无穷维的向量（特征函数）\\

&\lambda 表示特征值
\end{aligned}
$$

$$
\begin{aligned}
e^{-iwt}&满足\\
\Delta e^{-iwt} &= \frac{\partial^2 e^{-iwt} }{\partial t^2} = -w^2e^{-iwt}\\
所以e^{-iwt}&是变换\Delta 即拉普拉斯算子的特征函数。
\end{aligned}
$$

那么我们需要一个图上的拉普拉斯算子，然后求其特征函数，上面的拉普拉斯矩阵其实就是一个离散拉普拉斯算子，

[拉普拉斯矩阵与拉普拉斯算子的关系 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/85287578)

拉普拉斯算子是一个二阶微分算子，在离散情况下有：

$$
\begin{aligned}
一维情况下 &\Delta f = f(x+1) + f(x-1) - 2f(x)\\
二维网格情况下 &\Delta f = f(x+1, y) + f(x-1, y) + f(x, y+1) + f(x, y-1) - 4f(x, y)
\end{aligned}
$$

可以看出都是周围点与中心点的梯度差的和，那类比到图上就是中心点与邻居节点的梯度差的和。而拉普拉斯矩阵可以做到这点

$$
\begin{aligned}
(Lf)(i) &= \sum_{j\in N_i}^{}W_{ij}[f(i) - f(j)] \\
Proof: &\\
\because  Lf &= (D-W)f = Df - Wf\\
\therefore (Lf)(i) &= d(i)f(i) - \sum_{j\in N_i}^{}W_{ij}f(j) \\
&= \sum_{j\in N_i}^{}W_{ij}f(i) - \sum_{j\in N_i}^{}W_{ij}f(j) \\
&= \sum_{j\in N_i}^{}W_{ij}[f(i)-f(j)]
\end{aligned}
$$

所以可以将拉普拉斯矩阵看作图上的拉普拉斯算子，

根据拉普拉斯矩阵的特征分解，给出特征方程：

$$
\begin{aligned}
L = U\Lambda U^{-1} = U\Lambda U^{T}\\
LU = U\Lambda = \lambda U
\end{aligned}
$$

所以U为特征向量，类比傅里叶变换里的积分操作（求和），傅里叶变换及其逆变换为：

$$
\begin{aligned}
傅里叶变换\hat{f} = U^Tf\\
傅里叶逆变换f = U \hat{f}
\end{aligned}
$$

#### 卷积定理及图卷积网络

卷积定理：函数（向量）卷积的傅里叶变换是函数（向量）傅里叶变换的乘积。

那么在图上我们对一个信号f做卷积时，由于空间域的卷积不好做，我们可以利用卷积定理做谱（频域）上的乘积。

设空间域上的卷积核为h, 则由卷积定理

$$
\begin{aligned}
&(f\star h)_G = U((U^Th)\bigodot (U^Tf))\\
&设空间域上的卷积核h在图上的傅里叶变换为\hat{h}\\
&(f\star h)_G = U\hat{h}(U^Tf)\\
&\hat{h}为diag(U^Th),主要为了简化公式，防止出现hadamard\ product\bigodot
\end{aligned}
$$

###### 第一代GCN

$$
\begin{aligned}
&y = \sigma(Ug_{\theta}(\Lambda)U^Tx)\\
&其中g_{\theta}(\Lambda)为待训练的参数，是一个对角矩阵, diag([\theta_1, \theta_2, ..., \theta_n])。
\end{aligned}
$$

论文 2013 - Spectral Networks and Locally Connected Networks on Graphs - Bruna et al.

第一代GCN基本上就是上面卷积定理的翻版，直接将卷积核的傅里叶变换作为训练参数。但是，这样缺点明显，每一次前向传播的计算量大，且参数数量为n个, 与网络规模有关。

###### 第二代GCN

$$
\begin{aligned}
&设参数g_{\theta}(\Lambda) = \sum_{k=0}^{K-1}\theta_k \Lambda^k\\
&则Ug_{\theta}(\Lambda)U^T = \sum_{k=0}^{K-1}\theta_k L^k, 注意U\Lambda^kU^T = L^k \\
&最后y = \sigma(\sum_{k=0}^{K-1}\theta_k L^kx)

\end{aligned}$$

论文 2016 - Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering - Defferrard, Bresson, Vandergheynst

参数减少为k个，与网络规模无关，但是计算复杂度依然很大，因为要计算矩阵的幂。

###### 第三代GCN

论文 2016 - Semi-Supervised Classification with Graph Convolutional Networks - Kipf, Welling

$$
\begin{aligned}
&g_{\theta}(\Lambda) =\sum_{k=0}^{K-1}\theta_k T_k(\widetilde{\Lambda}), 其中\widetilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I_N \\
&T_k(x)为切比雪夫多项式，是一个迭代构造的多项式\\
&T_0(x)=1, T_1(x)=x, T_k(x)=2xT_{k-1}(x)-T_{k-2}(x) \\
&矩阵形式下T_0(\widetilde{\Lambda})=I, T_1(\widetilde{\Lambda})=\widetilde{\Lambda}, T_k(\widetilde{\Lambda})=2\widetilde{\Lambda}T_{k-1}(\widetilde{\Lambda})-T_{k-2}(\widetilde{\Lambda})\\
&最后y = \sigma(\sum_{k=0}^{K-1}\theta_k T_k(\widetilde{L})x)

\end{aligned}$$

在这篇论文中，作者限制K=2（只有两项，原文中K=1是因为这里和原文公式中求和的上限不一样）。并近似最大特征值为2，通过重整化的技巧，简化了算法在编程时的困难，并适用于普遍的图信号，及一个节点的信号是一个向量，而不是一个值，这样就可以进行网络嵌入。

$$
\begin{aligned}
&Z = softmax(\hat{A}ReLU(\hat{A}XW^0)W^1)\\
&\hat{A}=\widetilde{D}^{-1/2}\widetilde{A}\widetilde{D}^{-1/2}\\
&\widetilde{A} = A + I_N \\
&\widetilde{D}_{ii} = \sum_{j}\widetilde{A}_{ij}

\end{aligned}$$

W是可训练的参数，可以通过W调节层与层之间节点向量维度的变化。